---
title: "BUDA525Final"
output: html_document
Team Members: Kristen Hopkins, Jacob Sossamon, Taggart Shea
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 3 (30 Points)

#In the `Credit` data in the `ISLR` package it contains 400 customers and information on their credit history. For full information of the data look at the `help` file. A company has approached us to better understand factors that influence the `Balance` variable which is average credit card balance in USD. Using the information in the model discuss the influential factors, and discuss the factors you choose to put in the model. Do you have any concerns about the use of certain variables in the model? Discuss how your model was created and any insights you can provide based on the results. HINT: Adding Gender and/or Ethnicity could be controversial or illegal in some uses of this this model you should discuss your decision on these variables and how it effects the organizations ability to use your model for prediction or inference.

Exploring the data
```{r}
library(ISLR)
data("Credit")
help(Credit)
head(Credit)
summary(Credit)
plot(Credit, main="Credit", col.main="#EAAA00",col="#002855")
```
From plotting the credit data, we can immediately see that the rating and limit appear to be visibly positively correlated with balance. There could be some multicollinearity concerns though, with having both the limit and rating variables in the model, since they look almost perfectly correlated with each other. This is understandable as individuals with higher credit scores usually have higher credit limits.

We are going to remove the ID, Gender, and Ethnicity variables from contention in our models because ID will be useless and gender and ethnicity could create ethical or legal problems if our models are using them to predict balances. The data we are using to train these models could be already biased against gender or ethnicity, so its best to just leave them out completely.

It is also possible that some of the variables may need transformed such as the limit and balance variables where there is a very wide range of values.

Seeing how many balances are averaged at 0 and viewing the distribution of balances.
```{r}
par(mfrow=c(1,1))
hist(Credit$Balance, main= "Histogram of avg CC balance in $", col.main="#002855", col.lab="#002855", xlab = "Avg CC balance in $$", col="#EAAA00")

nrow(Credit[Credit$Balance == 0,])
```
As you can see, almost a quarter of the data set has balances with values of 0. As a result, we are going to exclude all of the averaged out balances with values of 0 since the large number of these present in our data could negatively impact our models and skew results. It is intuitive to exclude these balances because a credit card company would most likely be interested in knowing the average balances across their frequent users, not ones who have not used their cards in a significant amount of time, leading them to have averaged out balances of 0 for the period.

```{r}
attach(Credit)
```

Removing unwanted columns and removing all of the balances with 0.
```{r}
Credit <- subset(Credit, select = -c(Ethnicity, Gender, ID))

Credit <- Credit[Credit$Balance > 0, ]

attach(Credit)

hist(Credit$Balance,, main= "Histogram of avg CC balance in $ (no $0 balances or unwanted columns)", col.main="#002855", col.lab="#002855", xlab = "Avg CC balance in $$", col="#EAAA00")
```

Performing backwards elimination with AIC:

Default Model (all predictor variables, no transformations)
```{r}
#model using ALL predictor variables
mod1 <- lm(Balance~Income+Limit+Rating+Cards+Age+Education+Student+Married)


#backwards selection on model 1
mod1S <- step(mod1)
mod1SB <-step(mod1, k=log(100))

summary(mod1S)
summary(mod1SB)


```
mod1S gives us an AIC of 1462.3 and tells us the best model we can come up with doesn't use education as a variable. Using mod1SB to look at the BIC, it tells us to remove both married and education as a variable. But, beause the score is higher at 1481.7 and we think that the married variable might contribute to our model, we decided to just keep it, and possibly transform it to be more significant in our model. 

Looking at the summary of mod1S, all other variables are significant at the .05 level or better except for Married at 0.1. The adjusted r squared value is 0.99

Using an automatic selection of transformation and boxCox to see if any log transformations would help our model.
```{r}
install.packages("car")
library(car)

summary(powerTransform(cbind(Balance,Rating,Limit,Income,Cards,Age,Education)))

boxCox(mod1S)
```
This tells us that no log transformations would improve our model.

Looking at the residuals
```{r}
#the backwards elimination model
par(mfrow=c(2,2))
plot(mod1S, col.main="#EAAA00", col.lab="#EAAA00", main = "mod1S Residuals", col="#002855")
```

Removing one of the highly correlated variables (rating and limit), in this case we decided to drop credit rating
```{r}
mod2 <- lm(Balance~Income+Limit+Cards+Age+Student+Married) 

summary(mod1S)
summary(mod2)
```
The output of mod2 seems to be very similar to mod1S, just with the coefficients slightly different, and this time married has more significance on the model. The standard errors are also slightly less. Hopefully by removing one of the highly correlated variables, the model will have better interpretability and more reliable coefficient estimates.

```{r}
par(mfrow=c(2,4))
plot(mod2, main= "mod2", col.main="#002855", col = "#EAAA00", col.lab="#002855")
plot(mod1S, main="mod1S", col.main="#EAAA00", col= "#002855", col.lab="#EAAA00")
```
We can see our residuals seem to be better for mod2 as well. 

Trying to clean up the residual plots of mod2 even more (data point 247 and 192 have very high balances, could be throwing off plot a little)
```{r}
Credit[247,]
Credit[192,]

Credit <- Credit[-247,]
Credit <- Credit[-192,]
```
Fitting model after removing those two rows:
```{r}
attach(Credit)

mod3 <- lm(Balance~Income+Limit+Cards+Age+Student+Married) 

summary(mod3)
```
The residual spread went from -34.4 - 23.9 to -28.2 - 23.9 by removing those two points (not necessary but we thought we might as well experiment)

```{r}
par(mfrow=c(2,4))
plot(mod3, main= "mod3", col.main="#002855", col = "#EAAA00", col.lab="#002855")
plot(mod2, main="mod2", col.main="#EAAA00", col= "#002855", col.lab="#EAAA00")
```

```{r}
par(mfrow=c(1,2))
residualPlot(mod2, main="mod2 Residual Plot", col.main="#EAAA00", col= "#002855", col.lab="#EAAA00")
residualPlot(mod3, main="mod3 Residual Plot", col.main="#002855", col = "#EAAA00", col.lab="#002855")
```
You can see the two points in the bottom right corner went away in the mod3 which improved the slight curve of the line we had before in mod2. 

Seeing if we can remove even more predictor variables to have a simpler model:
```{r}
#Submodel without Age and Married variables
mod4 <- lm(Balance~Income+Limit+Cards+Student)

anova(mod4,mod3)
```
Based on the anova table comparing model 4 and model 3, we see that we can reject the null hypothesis of the simpler model being better in this case because the p value is very small, thus telling us that the Student and Married variables should stay in the model.

Summary:

We chose to use backwards elimination to select the best predictors for our model. it provided us with a model that used Income, Limit, Rating, Cards, Age, Student, and Married. We then manually removed Rating since Limit essentially accounts for all that Rating offers. We then took away two potential influence points to clean up our residuals even more to give us our final model. Finally we tested if we could simplify the model any further by removing two variables and found that we should not.

Interpretation:

```{r}
summary(mod3)
```

Our model allows us to view the influential factors that affect credit card balances for this particular set of data. We can also see what specific effects the predictor variables have on balance by looking at the coefficients. The income variable has a negative correlation on balance, which seems somewhat counter-intuitive because we would assume the more income you have, the more you would spend leading to higher average credit card balances. But, we could think about this a different way like because people have more income, they can consistently pay off their credit card balance, keeping a low avgerage balance overall. So, this model predicts a lower balance, the higher a persons income.

Next we have Limit, where a higher credit limit results in a higher card balance. This definitely makes sense to us, the higher your limit the more you can spend. For the card variable, our model tells us that the more cards someone has, the higher their average card balance will be. For age, the older the person is, the less their balance will be, indicating that younger people tend to spend more on average, or maybe they can't pay off their balances as much as older people. The final two variables tell us that students generally have higher card balances and that married people tend to have lower balances. This one could be due to a married couple dividing their total household spending among two people.

Prediction example:

To test our model we are going to predict a card balance for someone that: 
Has $100,000 income
An $8,000 credit limit
2 Cards
Is 35 years old
Is not a student
Is not Married
```{r}
predict(mod3,newdata=data.frame(Income=100, Limit=8000, Cards=2, Age=35, Student="No", Married="No"))
```
The model predicts that this individual would have an averaged out credit card balance of $927.1 for the period.








## Problem 4 (30 Points)

The `Salaries` data in the `carData` package contains information on academic salaries in 2008 and 2009 in a college in the US. A data dictionary can be found in the `help` file for the data. This data was collected as part of an on-going effort of the college to monitor salary differences between male and female faculty members. We have been asked to investigate the gender gap in the data, but also what other information that may be relevant to administrators (i.e. salary growth for years of service, discipline based growth, etc). Investigate if there is a gender gap, but also provide insights on other drivers that you may see of salary in the data. Is your model suitable to make offers based on the information provided? Explain your reasoning. Provide insights into any other information you find of interest.

Exploring the data
```{r}
library(carData)
data("Salaries")
help(Salaries)
summary(Salaries)
head(Salaries)
```
Some potential concerns right away that we see from the data set is that there are 358 male observations and only 39 female. Also there is a lot more professor ranks at 266 than assistant and associate professors at 67 and 64 in the data set.

```{r}
attach(Salaries)
```
Splitting data into different subsets
```{r}
SalariesRank <- split(Salaries, rank)

Professors <- SalariesRank$Prof
AsstProfessors <- SalariesRank$AsstProf
AssocProfessors <- SalariesRank$AssocProf

SalariesSex <- split(Salaries, sex)

Males <- SalariesSex$Male
Females <- SalariesSex$Female
```

Calculating the means of each rank by sex
```{r}
cat("Professors","\n")
tapply(Professors$salary,Professors$sex,mean)
cat("\n","Assistant Professors","\n")
tapply(AsstProfessors$salary,AsstProfessors$sex,mean)
cat("\n","Associate Professors","\n")
tapply(AssocProfessors$salary,AssocProfessors$sex,mean)
```
The results of these calculations provide interesting results. They show that there is at least a $3,000 difference between the averages of male salaries and female salaries in each of the 3 ranks, indicating a gender pay gap may be present.

Creating a model using only sex as a predictor
```{r}
mod4 <- lm(salary~sex, data=Salaries)

summary(mod4)
```
Again we see that from this model, it predicts on average males would make $14,088 more than females, and that the sex variable is statistically significant. However, the model only explains a little less than 2% of the total variation with an adjusted r squared score of 0.01673.

Performing a t-test hypothesis test similar to the model we just looked at
```{r}
t.test(Salaries$salary[Salaries$sex=="Male"], Salaries$salary[Salaries$sex=="Female"], alternative="greater")
```
The t-test shows the same result as the model we just created, where males on average have $14,088 higher salaries than females. The hypothesis test tells us that if the null hypothesis were true and there was no real difference between male and female salaries, the probability of observing a value as extreme as our data would be 0.001, indicating we should reject the null hypothesis and recognize the higher average salary of males in this case.


Using backwards elimination to find the recommended number of predictor variables based on AIC
```{r}
mod5 <- lm(salary~.,data=Salaries)
mod5S <- step(mod5)

summary(mod5S)
```

Analyzing the impact of the sex variable using anova
```{r}
anova(mod5,mod5S)
```

The result of the backwards elimination and the anova table shows that the sex variable is not needed and that the other 4 variables are better predictors of salary. The sex variable does not significantly improve the model's ability to explain variance in salary when rank, discipline, yrs.since.phd, and yrs.service are already accounted for.

Checking the residuals
```{r}
par(mfrow=c(2,2))
plot(mod5S, main="mod5", col.main="#002855", col = "#EAAA00", col.lab="#002855")
```


Looking at other factors:


Checking if there is a promotion discrepancy by creating models for predicting the number of years someone has been there to what rank they are
```{r}
mod7 <- lm(Males$yrs.service~Males$rank)
mod7b <- lm(Females$yrs.service~Females$rank)

summary(mod7)
summary(mod7b)
```
The result of these two summaries shows that according to this data set, Females actually obtain higher positions in less years than Males. for example, the average number of years in service for the associate professor rank for males is 9.7 years but only 8.96 for females. And, the average number of years in service for the professor rank for males is 20.89 for males and only 14.57 for females.

Checking if there is a difference in salaries between disciplines
```{r}
t.test(Salaries$salary[Salaries$discipline=="B"], Salaries$salary[Salaries$discipline=="A"], alternative="greater")
```
This hypothesis test tells us that employees teaching in the applied departments (B) have higher average salaries than those in the theoretical departments (A).






