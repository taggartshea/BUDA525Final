---
title: "BUDA525Final"
output: html_document
Team Members: Kristen Hopkins, Jacob Sossamon, Taggart Shea
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 3 (30 Points)

#In the `Credit` data in the `ISLR` package it contains 400 customers and information on their credit history. For full information of the data look at the `help` file. A company has approached us to better understand factors that influence the `Balance` variable which is average credit card balance in USD. Using the information in the model discuss the influential factors, and discuss the factors you choose to put in the model. Do you have any concerns about the use of certain variables in the model? Discuss how your model was created and any insights you can provide based on the results. HINT: Adding Gender and/or Ethnicity could be controversial or illegal in some uses of this this model you should discuss your decision on these variables and how it effects the organizations ability to use your model for prediction or inference.

Exploring the data

```{r}
library(ISLR)
data("Credit")
help(Credit)
head(Credit)
summary(Credit)
plot(Credit)
```

From plotting the credit data, we can immediately see that the rating, limit, and income variables are all visibly correlated with balance. The variables like student and married are harder to tell just from a plot because they are factor variables. We are going to remove the ID, Gender, and Etnicity variables from contention in our models because ID will be useless and gender and ethnicity could create ethical or legal problems if our models are using them to predict balances. The data we are using to train these models could be biased against gender or ethnicity, so its best to just leave them out completely.

It is also possible that some of the variables may need transformed such as the limit and balance variables where there is a very wide range of values.

```{r}
attach(Credit)
```

Removing unwanted columns and adding 1 to the balance fields so there are no 0's for transformations

```{r}
Credit <- subset(Credit, select = -c(Ethnicity, Gender, ID))

Credit$Balance <- Credit$Balance + 1

summary(Credit)
```

Performing backwards elimination with AIC:

Default Model

```{r}
#model using ALL predictor variables
mod1 <- lm(Balance~.,data=Credit)

#backwards selection on model 1
mod1S <- step(mod1)

summary(mod1S)
```

Gives us an AIC of 3679.9

Examples of potential transformation that would help residuals

```{r}
plot(Balance~Rating)
plot(log(Balance)~log(Rating))

plot(Balance~Limit)
plot(log(Balance)~log(Limit))

plot(Balance~Income)
plot(log(Balance)~log(Income))
```

Transformed Models

```{r}
#Model using ALL variables, but log transformations for limit, rating, and Income, and log transformation for Balance
mod2 <- lm(log(Balance)~log(Limit)+log(Rating)+log(Income)+Education+Age+Married, data=Credit)

#Model using ALL variables, but log transformations for only Income (suggested by the powertransform function)
mod2b <- lm(Balance~Limit+Rating+log(Income)+Education+Age+Married, data=Credit)

#backwards selection on both transformed models
mod2S <- step(mod2)
mod2bS <- step(mod2b)

summary(mod2S)
summary(mod2bS)
```

Gives us an AIC of 181.19 and 4160.6

Polynomial Model
```{r}
#model using polynomial continuous variables
mod3 <- lm(Balance~poly(Income+Limit+Rating+Cards+Age+Education, 2, raw=TRUE), data=Credit)

#backwards selection on model 3
mod3S <- step(mod3)

summary(mod3S)
```

Gives us an AIC of 4365.4

Comparing all of the residual plots

```{r}
#the default model
par(mfrow=c(2,2))
plot(mod1S)

#Model with log transformation of Balance, Income, Rating, and Limit
par(mfrow=c(2,2))
plot(mod2S)

#Model with log transformation of just Income
par(mfrow=c(2,2))
plot(mod2bS)

#Polynomial Model
par(mfrow=c(2,2))
plot(mod3S)
```

summary:

The default model after backwards elimination had an AIC of **3679.9** and its residual plot looks very bad showing significant curvature.

The second model, using log transformation of balance, income, rating, and limit, after backwards elimination had an AIC of **181.19** and its residual plot looks very bad as well.

The other variation of the second model using the log transformation of just income, after backwards elimination had an AIC of **4160.6** and its residual plot looks a little better with a tighter spread and flatter line.

Finally, the third model, using polynomial regression, after backwards elimination had an AIC of **4365.4** and the best looking residual plot, appearing closest to a null plot just with a strange sloped line of residuals near the lower fitted values.


Interpretation:

Model 2 has the lowest AIC score which means it is a good fit for our data, but maybe not as complex as we want it to be. Because the residuals are not the best on this model, we probably will want to use the thrid model to make our predictions, due to the absence of NCV and our curves being flat.




## Problem 4 (30 Points)

The `Salaries` data in the `carData` package contains information on academic salaries in 2008 and 2009 in a college in the US. A data dictionary can be found in the `help` file for the data. This data was collected as part of an on-going effort of the college to monitor salary differences between male and female faculty members. We have been asked to investigate the gender gap in the data, but also what other information that may be relevant to administrators (i.e. salary growth for years of service, discipline based growth, etc). Investigate if there is a gender gap, but also provide insights on other drivers that you may see of salary in the data. Is your model suitable to make offers based on the information provided? Explain your reasoning. Provide insights into any other information you find of interest.

Exploring the data
```{r}
library(carData)
library(car)
data("Salaries")
help(Salaries)
summary(Salaries)
head(Salaries)
```
Some potential concerns right away that we see from the data set is that there are 358 male observations and only 39 female. Also there is a lot more professor ranks at 266 than assistant and associate professors at 67 and 64 in the data set.

```{r}
attach(Salaries)
```
Splitting data into different subsets
```{r}
SalariesRank <- split(Salaries, rank)

Professors <- SalariesRank$Prof
AsstProfessors <- SalariesRank$AsstProf
AssocProfessors <- SalariesRank$AssocProf

SalariesSex <- split(Salaries, sex)

Males <- SalariesSex$Male
Females <- SalariesSex$Female
```

Calculating the means of each rank by sex
```{r}
cat("Professors","\n")
tapply(Professors$salary,Professors$sex,mean)
cat("\n","Assistant Professors","\n")
tapply(AsstProfessors$salary,AsstProfessors$sex,mean)
cat("\n","Associate Professors","\n")
tapply(AssocProfessors$salary,AssocProfessors$sex,mean)
```
The results of these calculations provide interesting results. They show that there is at least a $3,000 difference between the averages of male salaries and female salaries in each of the 3 ranks, indicating a gender pay gap may be present.

Creating a model using only sex as a predictor
```{r}
mod4 <- lm(salary~sex, data=Salaries)

summary(mod4)
```
Again we see that from this model, it predicts on average males would make $14,088 more than females, and that the sex variable is statistically significant. However, the model only explains a little less than 2% of the total variation with an adjusted r squared score of 0.01673.

Performing a t-test hypothesis test similar to the model we just looked at
```{r}
t.test(Salaries$salary[Salaries$sex=="Male"], Salaries$salary[Salaries$sex=="Female"], alternative="greater")
```
The t-test shows the same result as the model we just created, where males on average have $14,088 higher salaries than females. The hypothesis test tells us that if the null hypothesis were true and there was no real difference between male and female salaries, the probability of observing a value as extreme as our data would be 0.001, indicating we should reject the null hypothesis and recognize the higher average salary of males in this case.


Using backwards elimination to find the recommended number of predictor variables based on AIC
```{r}
mod5 <- lm(salary~.,data=Salaries)
mod5S <- step(mod5)

summary(mod5S)
```
The result of the backwards elimination shows that the sex variable is not needed and that the other 4 variables are better predictors of salary.

Checking the residuals
```{r}
par(mfrow=c(2,2))
plot(mod5S)
```




Looking at other factors:


checking if there is a promotion discrepancy by creating models for predicting the number of years someone has been there to what rank they are
```{r}
mod7 <- lm(Males$yrs.service~Males$rank)
mod7b <- lm(Females$yrs.service~Females$rank)

summary(mod7)
summary(mod7b)
```
The result of these two summaries shows that according to this data set, Females actually obtain higher positions in less years than Males. for example, the average number of years in service for the associate professor rank for males is 9.7 years but only 8.96 for females. And, the average number of years in service for the professor rank for males is 20.89 for males and only 14.57 for females.

Checking if there is a difference in salaries between disciplines
```{r}
t.test(Salaries$salary[Salaries$discipline=="B"], Salaries$salary[Salaries$discipline=="A"], alternative="greater")
```
This hypothesis test tells us that employees teaching in the applied departments (B) have higher average salaries than those in the theoretical departments (A).






